# Task_01 Notes

## 1概念

机器学习：计算机通过学习算法，在海量的数据中寻找数学规律，并输出符合数据分布的模型。

Data Set（数据集）：一个关于某个事物或者对象的特征的描述的集合。集合里面每一个被称为样本（Sample）或者样例（instance）。例如：

特征（feature）、属性（attribute）：所描述对象或事物的性质或者属性。例如：”颜色“，”形状“等。

属性值（attribute value）：关于特征与属性的取值。例如：“青绿”、“椭圆”。

样本空间（sample space）、属性空间（attribute space）：由多个特征或属性组成的多维空间。每个特征作为一个维度的坐标轴。这样描述可以使得每个样本在空间里面有一个唯一对应的坐标点，所以样本也被叫做，“特征向量”（feature vector）。

学习（learning）、训练（training）：从数据中，使用学习方法获取模型的过程。

训练数据（training data）、训练样本（training sample）、训练集（training set）

假设（hypothesis）：假设学习的数据符合某种潜在的分布。

真相（ground-truth）：假设中潜在的规律。学习的过程就是找到最接近真相的模型。

标记（label）：样本的结果信息。“标记空间”

样例（example）：带有（label）的样本。

分类（classification）：模型根据输入的数据，输出离散预测值。二分类“binary classification”、多分类“Multi-class classification”

回归（regression）：模型根据输入的数据，输出连续预测值。

测试（test）：训练得到数据后，使用模型进行预测的过程。

测试样本（test sample）：被预测的样本，不带label。

聚类（cluster）：由于学习的样本没有标记，学习算法对这些样本进行自动分类。

泛化能力（generalization）：学习的模型，适应于新样本的能力。

独立同分布（independent and identically distributed，i.i.d）：在概率统计理论中，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。

不同版本的假设空间输出的模型会产生不同的模型，学习就是就在所有的假设空间中搜索与训练集能够匹配上的过程。但是现实问题中，常常存在较大的假设空间，而训练的样本是有限的，从而有可能同一个训练集会有多个假设空间符合。最终导致误判。

归纳偏好（inductive bias）：机器学习算法在学习过程中对某种类型假设的偏好。也就是选择某一类“较容易识别”特征进行假设，完全取决于用户的偏好。

有可能发生的情况是：在某种假设情况下，能输出泛化能力很强的模型，在另一种假设的情况下，表现很糟糕。

## 2模型评估与选择

### 2.1 经验误差与过拟合

模型评估常用指标：错误率（error rate）、精度（accuracy）、误差（error）、训练误差（training error）|经验误差（empirical error）。

泛化误差（generalization error）：模型在新样本上得到的误差。

常常，由于我们事先不知道新样本是什么样的，在训练模型时，只能尽量选择优化训练误差。实际上我们想要的是能够在新样本上，表现的好的模型，为了达到这样的目的，应该在训练时，模型应尽可能的学习训练样本中所有潜在样本都存在的一般规律、一般性质。

如果过度关注于训练集中训练样本自身存在的特有性质，导致模型把特有性质当做了所有潜在样本的一般规律就会发生——过拟合"overfitting"，最终导致模型的泛化性能下降。“欠拟合”（underfitting）与之相反，样本的一般性质并没有被模型很好的识别到。

在学习和分析机器学习的算法时，一般都关注，该算法是如何缓解“过拟合”现象的，并且在什么情况下，该方法会失效。“过拟合”问题是机器学习面临的主要问题。

通常地，在实际任务中，对于同一个问题，会有多种算法可供选择，不同的参数配置方案，这样就导致了同一个问题会有多种的解决方案。理想的解决方案是对候选的模型的泛化误差进行评估，选择出泛化误差最小的模型。但是，由于无法直接获取泛化误差，又不可选择训练误差作为评估标准。

### 2.2 评估方法

一般地，我们可以通过实验测试每个模型的泛化误差进行评估。使用“测试集”（test set)测试模型的对新样本的判别能力，把测试集的"测试误差"（test error）近似当做泛化误差进行评估。

在选择测试集时，一般与训练集一样，从样本真实分布中，独立同分布采样获得——为了保证测试样本的分布与真实分布一致。但需注意的是，测试样本不能与训练集中的样本一样，测试集应尽量与训练集互斥。

#### 2.2.1 留出法

“留出法”（hold-out）：把所拥有的数据集按照比例分为两个互斥的训练集与测试集。100次实验

注意：

- 划分时应该注意保持数据分布的一致性，“分层采样”（stratified sampling），即：在原数据集中，各类样本所占比例为多少，划分后，各类样本的所占比例应该是一致的。
- 多次重复划分，对原数据集划分多次，得到不同的训练集与测试集，然后把多次模型评估后的结果取平均值。把因由于数据集划分对结果造成的影响给平均掉。一般为100次随机划分。
- 测试集不能太大、太小。一般占1/5-1/3。训练集的数据越大，那么训练出来的模型越接近用原数据集训练出来的模型，但同时会降低准确性。如果测试集越大，那么会降低模型评估的真实性（fidelity）。没有完美的解决方案。
- 有可能数据没有被用作测试。但是，可能该数据比较重要。

#### 2.2.2 交叉验证法

“交叉验证”（k-fold cross validation）、：

![image-20221213104526675](E:\Robotic system engineering\pumpkin-book-notes\Task01\图片材料\2.2.2.png)

把数据集D划分为K个互斥子集，每个子集随机分层采样。把K-1个子集作为训练集，剩余一个作为测试集，进行K次实验测试，该方法的评估结果稳定与保真性极大地取决于K的值。一般取K=10，同样地由于是随机划分子集，也有上述的问题——样本划分不同引起的评估结果差异，所以选择多次划分。一般为10次。

留一法：D中有m个样本，k=m，只剩一个样本进行评估测试，这样训练出来的模型与D中样本分布的规律极其相似。

存在的问题：

- 单一性。
- 样本多时，计算量大。

#### 2.2.3 自助法

“自助法”（bootstrap）J基于“有放回采样”、“可重复采样”的采样方法：对于一个包含==m==个样本的数据集D,我们对它进行采样生成数据集D^1^：每次从数据集中挑选一个样本x~i~，把x~i~复制到数据集D^,^{x~j~}中即为x~i~=x~i~，同时放回样本x~i~到数据集D中，使得该样本能够在下次采样时被选到；这样的采样过程重复==m==次，我们就得到了包含m个样本的数据集D^1^。显然，在选择采样的过程中会有一部分数据，一直没有被采样过，这一部分的概率为：
$$
\lim _{m \mapsto \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368
$$
这样实际评估的模型与真实评估的模型都使用了m个样本用于==训练模型==，但是仍有部分样本没有被使用到，而仍有1/3的样本没有被采样的样本用于测试模型。这样的测试结果被称为“包外估计”（out of eliminate）。

适用于数据集较小、难以有效划分训练的情况，适用于集成学习。

但是该方法改变了原数据集中样本原本的分布，这会产生估计误差。 

#### 2.2.4 调参与最终模型

大多数算法都有参数（parameter）需要设定，参数配置的不同，学习训练得到的模型性能也不一样。在进行模型评估和选择时，除了要选择不同的算法，也要对算法的参数进行选择。

“超参数”：人工选择

“参数”：算法自己学习

选择算法：选择不同类型的算法。

参数调节：参数在实数范围进行选择，常用做法是：确定参数值的范围，以一定的步长去迭代，验证。

对于m个样本的数据集D，划分训练集n与测试集d，还需要从训练集中划分一部分h个样本“验证集”（validation set），用训练集中剩余的n-d个样本用于训练算法中模型的超参数，验证集中的h个样本来评估由于不同超参数输出模型的性能。

选定超参数后，再用训练集n进行训练输出算法中需要算法自己学习的参数。

### 2.3 性能度量



### 2.4 比较检验
